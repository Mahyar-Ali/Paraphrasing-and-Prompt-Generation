{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prompt Engineering.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "h068_TY4-zVF",
        "_-nYoEYb_APa",
        "yvCPI9XoxiD2",
        "9TLfcj5geTow",
        "EyFAAm607jam",
        "sBAxRMPK3pcs",
        "i2Yj_vZIzrYu",
        "bkpB1oBC02fP"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvCPI9XoxiD2"
      },
      "source": [
        "## DialoGPT - Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd4Zi7zaxuJL"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhp_-JqGxmnV"
      },
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "3705059393ed4cc7958675b6961f7187",
            "b9ce69131ec945dcb2b70aa9cf5c2642",
            "40eb4c83a8b94c4880ac3a277d5fb85c",
            "e522709b00a94df3865c8733ba1d3bc3"
          ]
        },
        "id": "_0zUhGkAyaBc",
        "outputId": "f4168b19-1cd0-4cb9-8846-226af64ea445"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3705059393ed4cc7958675b6961f7187",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9ce69131ec945dcb2b70aa9cf5c2642",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/642 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40eb4c83a8b94c4880ac3a277d5fb85c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e522709b00a94df3865c8733ba1d3bc3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "625a5b4ff0374339b33254950b321f98"
          ]
        },
        "id": "eAnq6pn6ydqp",
        "outputId": "fa74bd35-9452-4bb3-f3f6-0de760ec508f"
      },
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "625a5b4ff0374339b33254950b321f98",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gsug94uUyg6P",
        "outputId": "1abd659d-7959-413b-a4c2-0cfc63f7ba10"
      },
      "source": [
        "step = 0\n",
        "new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
        "bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> User:Do you mind providing your email address?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYDWeTiG1eNe"
      },
      "source": [
        "# Using the same input and then passing it to DialoGPT over and over again without any history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tIzsqJxzbX_",
        "outputId": "436e6e83-32f2-4c74-f229-e1e44df04213"
      },
      "source": [
        "for i in range(19):\n",
        "  \n",
        "  chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id,\n",
        "          no_repeat_ngram_size=3,       \n",
        "          do_sample=True, \n",
        "          top_k=100, \n",
        "          top_p=0.7,\n",
        "          temperature=0.8)\n",
        "\n",
        "\n",
        "  print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DialoGPT: Just sent you a PM.\n",
            "DialoGPT: I PM'd you.\n",
            "DialoGPT: You can email me at daniel.co.uk\n",
            "DialoGPT: No problem, sent you a PM.\n",
            "DialoGPT: Yes please.\n",
            "DialoGPT: I don't mind at all. I'll PM you.\n",
            "DialoGPT: I'll send it to you now.\n",
            "DialoGPT: No problem, I sent you a message.\n",
            "DialoGPT: I sent you a PM\n",
            "DialoGPT: Just PM'd you.\n",
            "DialoGPT: No problem. I'll send it to you.\n",
            "DialoGPT: No problem. PM me.\n",
            "DialoGPT: PM sent.\n",
            "DialoGPT: PM sent.\n",
            "DialoGPT: Sure thing.\n",
            "DialoGPT: PM sent\n",
            "DialoGPT: I sent you a message.\n",
            "DialoGPT: I'm pretty sure that I sent you a PM with my email address.\n",
            "DialoGPT: Yes I'll send it to you via PM.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUIWezHn0mNe"
      },
      "source": [
        "# Let's see how the model responds if we ask it the same question over and over again but this time \n",
        "#we are providing it some history."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnpCG8a82lTn"
      },
      "source": [
        "prompt = 'Do you mind providing your email address?'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LcOYzmY2R6m",
        "outputId": "0e076d72-9302-45e8-88f1-7e84c657c29a"
      },
      "source": [
        "for step in range(10): \n",
        "  new_user_input_ids = tokenizer.encode(prompt + tokenizer.eos_token, return_tensors='pt')\n",
        "  bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
        "  \n",
        "  chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id,\n",
        "          no_repeat_ngram_size=3,       \n",
        "          do_sample=True, \n",
        "          top_k=100, \n",
        "          top_p=0.7,\n",
        "          temperature=0.8)\n",
        "\n",
        " \n",
        "  print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DialoGPT: No problem, I'll PM you.\n",
            "DialoGPT: It's a joke.\n",
            "DialoGPT: Do u mind providing a response to my email address?\n",
            "DialoGPT: I'm pretty sure I already did\n",
            "DialoGPT: That's just what I do.\n",
            "DialoGPT: And you are not?\n",
            "DialoGPT: Sure, I will do that.\n",
            "DialoGPT: You are you kidding?\n",
            "DialoGPT: Heh?\n",
            "DialoGPT: Are you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHGJuUbQ2p7D"
      },
      "source": [
        "# well, I would have expected the same reply from someone if I kept asking for his email over and over again :-)\n",
        "\n",
        "# This one is not useful, I guess.\n",
        "# Let's play around with the values of temperature using the original approach (no history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xYe0o1z3N75",
        "outputId": "4cba40e2-06a5-4f7f-8002-68bc239a54ca"
      },
      "source": [
        "step = 0\n",
        "new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
        "bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> User:Do you mind providing your email address?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CysHAyP3blX",
        "outputId": "554e2026-7f16-4486-b97b-2564cf1d1e9f"
      },
      "source": [
        "for i in range(19):\n",
        "  \n",
        "  chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id,\n",
        "          no_repeat_ngram_size=3,       \n",
        "          do_sample=True, \n",
        "          top_k=100, \n",
        "          top_p=0.7,\n",
        "          temperature=1.0)\n",
        "\n",
        " \n",
        "  print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DialoGPT: sent you a PM.\n",
            "DialoGPT: Yes, it's a private message.\n",
            "DialoGPT: no problem! i sent you one.\n",
            "DialoGPT: It's a secret\n",
            "DialoGPT: Just sent it.\n",
            "DialoGPT: PM'd it to you\n",
            "DialoGPT: PM'd you\n",
            "DialoGPT: pm me\n",
            "DialoGPT: just sent it to you\n",
            "DialoGPT: Yeah sure, I'll PM you it.\n",
            "DialoGPT: Sure, what's yours?\n",
            "DialoGPT: PM sent.\n",
            "DialoGPT: Hey, yeah, I'll PM you it.\n",
            "DialoGPT: no, I'll PM it to you\n",
            "DialoGPT: Done, thanks!\n",
            "DialoGPT: Sure thing!\n",
            "DialoGPT: I'll send you a message.\n",
            "DialoGPT: PM'd you.\n",
            "DialoGPT: It's on the top of the page.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulj-U7503nRq",
        "outputId": "0892552e-f81d-40de-881a-db0004567c09"
      },
      "source": [
        "for i in range(10):\n",
        "  \n",
        "  chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id,\n",
        "          no_repeat_ngram_size=3,       \n",
        "          do_sample=True, \n",
        "          top_k=100, \n",
        "          top_p=0.7,\n",
        "          temperature=0.6)\n",
        "\n",
        "  print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DialoGPT: PM sent\n",
            "DialoGPT: PM sent.\n",
            "DialoGPT: I sent you a PM\n",
            "DialoGPT: I sent you a PM.\n",
            "DialoGPT: Sure, I'll PM it to you.\n",
            "DialoGPT: PM sent.\n",
            "DialoGPT: Sent you a PM.\n",
            "DialoGPT: PM sent.\n",
            "DialoGPT: Sent you a PM\n",
            "DialoGPT: I sent you a PM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHy-ikTk6nFj"
      },
      "source": [
        "# so high temperature results in more diverse and creative answers even though it decreases its confidence\n",
        "# in its top choices\n",
        "\n",
        "# \"Lower temperatures make the model increasingly confident in its top choices, while temperatures greater than 1 decrease confidence.\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGLRiLwB75c8",
        "outputId": "3eff134f-b827-41a5-c419-615deb386ff4"
      },
      "source": [
        "# let's turn off the sampling\n",
        "for i in range(10):\n",
        "   \n",
        "  chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id,\n",
        "          no_repeat_ngram_size=3,       \n",
        "          do_sample=False, \n",
        "          top_k=100, \n",
        "          top_p=0.7,\n",
        "          temperature=0.9)\n",
        "\n",
        "  print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DialoGPT: I sent you a PM.\n",
            "DialoGPT: I sent you a PM.\n",
            "DialoGPT: I sent you a PM.\n",
            "DialoGPT: I sent you a PM.\n",
            "DialoGPT: I sent you a PM.\n",
            "DialoGPT: I sent you a PM.\n",
            "DialoGPT: I sent you a PM.\n",
            "DialoGPT: I sent you a PM.\n",
            "DialoGPT: I sent you a PM.\n",
            "DialoGPT: I sent you a PM.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS9R-S7L8UqP"
      },
      "source": [
        "# as expected, as sampling is off, same answer will be generated everytime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGf9_Ouo8cIS",
        "outputId": "8ad5c495-b36f-4a69-e364-37ef27be4451"
      },
      "source": [
        "#let's play around with top_p and top_k\n",
        "for i in range(10):\n",
        "  chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id,\n",
        "          no_repeat_ngram_size=3,       \n",
        "          do_sample=True, \n",
        "          top_k=100, \n",
        "          top_p=0.9,\n",
        "          temperature=0.9)\n",
        "\n",
        "\n",
        "  print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DialoGPT: I think they just ask you to provide a username and email address\n",
            "DialoGPT: No problem. Sent you a message\n",
            "DialoGPT: I can send you the imgur link when i get home.\n",
            "DialoGPT: sent you a message\n",
            "DialoGPT: PM'd. Sorry for not getting back to you sooner!\n",
            "DialoGPT: Sure, sent you a PM.\n",
            "DialoGPT: Sure thing, PM me with your email.\n",
            "DialoGPT: Hey it's me, your email.\n",
            "DialoGPT: I'll PM you.\n",
            "DialoGPT: I've emailed you a code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWqw7kwd-pwW"
      },
      "source": [
        "# so top_p will help in increasing the length of the generated answer but as top p is quite high(0.9), increasing\n",
        "# top_k will quickly result in going off topic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhxgP0t5AS66"
      },
      "source": [
        "# In short- To generate diverse answers from DialoGPT, turn on the sampling, provide no history, increase the\n",
        "# temperature slightly and keep top_k in the range of 100 while maximizing top_p."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2Yj_vZIzrYu"
      },
      "source": [
        "## DialoGPT - Revisited - Functional Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OoKS9emzw8d"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oxhhoonmz91L"
      },
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBw2VEDr0Cde"
      },
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBqOLpFV0s8C"
      },
      "source": [
        "### Get Answers function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT-75y5mh2hb"
      },
      "source": [
        "def get_answer(prompts,hyperParameters,num_responses):\n",
        "  '''\n",
        "  prompts: List of input questions\n",
        "  hyperParameters: Dictionary of hyper-parameters\n",
        "  num_responses: Number of responses to generate for each question.\n",
        "  '''\n",
        "  \n",
        "  answers_with_prompt = []\n",
        "  for prompt in prompts:\n",
        "      print(prompt)\n",
        "      answers = []\n",
        "      step = 0\n",
        "      new_user_input_ids = tokenizer.encode(prompt + tokenizer.eos_token, return_tensors='pt').to(device)\n",
        "      bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
        "\n",
        "      if (hyperParameters['top_p_step'] is not None ):\n",
        "        for i in range(num_responses):\n",
        "          for top_p in np.arange(hyperParameters['top_p'][0],hyperParameters['top_p'][1],hyperParameters['top_p_step']):\n",
        "            chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id,\n",
        "                    no_repeat_ngram_size=3,       \n",
        "                    do_sample=hyperParameters['do_sample'], \n",
        "                    top_k=hyperParameters['top_k'], \n",
        "                    top_p=top_p,\n",
        "                    temperature=hyperParameters['temperature'])\n",
        "                    \n",
        "            answers.append(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True))\n",
        "      \n",
        "      else:\n",
        "        for i in range(num_responses):\n",
        "          chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id,\n",
        "                    no_repeat_ngram_size=3,       \n",
        "                    do_sample=hyperParameters['do_sample'], \n",
        "                    top_k=hyperParameters['top_k'], \n",
        "                    top_p=hyperParameters['top_p'],\n",
        "                    temperature=hyperParameters['temperature'])\n",
        "          answers.append(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True))\n",
        "      answers_with_prompt.append((answers,prompt))\n",
        "  return answers_with_prompt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkpB1oBC02fP"
      },
      "source": [
        "### Create CSV function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT04XwVrl_c7"
      },
      "source": [
        "def createCSV(paraphrased_answers_list, path, name_of_the_file):\n",
        "  '''\n",
        "  Parameters:-\n",
        "    paraphrases_list: output of the get_response function\n",
        "    path: path where the csv file should be stored\n",
        "    name_of_the_file: name  of the output csv file\n",
        "\n",
        "  '''\n",
        "  data = {'input_sentences':[],'output_sentences':[]}\n",
        "  for (answers, question) in paraphrased_answers_list:\n",
        "    rows_answers = answers\n",
        "    rows_questions = [question]* len(answers)\n",
        "    data['input_sentences'] += rows_questions\n",
        "    data['output_sentences'] += rows_answers\n",
        "  \n",
        "  dataFrame = pd.DataFrame(data, columns=['input_sentences','output_sentences'])\n",
        "\n",
        "  # removing duplicate answers\n",
        "  dataFrame = dataFrame.drop_duplicates(subset=['output_sentences'])\n",
        "  \n",
        "  dataFrame.to_csv(path+'/'+name_of_the_file)\n",
        "  return dataFrame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dg7oUPHYoB2g"
      },
      "source": [
        "### Hyper-Parameters Explanation\n",
        "\n",
        "**About DialoGPT**\n",
        "\n",
        "DialoGPT is formulated as an autoregressive (AR) language model. It is based on the mult-layer transformer architecture, much similar to that of GPT-2. DialoGPT is trained on 147M multi-turn dialogues extracted from Reddit discussion threads.\n",
        "\n",
        "----------------------------------------------------------\n",
        "**Hyper-Parameters:-**\n",
        "\n",
        "***-> Temperature:*** Temperature sampling is implemented by dividing logits by the temperature value before feeding them into softmax. Temperature greater than 1 generally decreases the confidence while lower temperature makes the model increasingly confident in its top choices. Therefore, we have to find a balance for the value of temperature. If we decrease the temperature too much, the model will become too confident in its top choices and will result in genetating the same answer again and again. On the other hand, if we increase the temperature too much, it will start diverting from the original intent. A value of `0.8` was wound to be perfect for our case (The value was choosen after testing multiple values.)\n",
        "\n",
        "----------------------------------------------------------\n",
        "***-> Top_k and Top_p:*** Top_k means that we are sorting by probability and then zero-ing out the probabilities for anything below the k'th token. In short, sort the output tokens by probabilities, select top k, and then disregard everything that doesn't fall in the first k output tokens. This can reduce the probability space from which we are sampling our output because we are selecting the top k outs and then disregarding everything else. If K is small, this can result in less diversity and variation of the output answers. We can make the top_k large but that can result in incuding totally off topic words. \n",
        "\n",
        "To address the above problems, top_p is used. It introdcues commulative distribution. As soon as the Commulative Distribution Frequency exceeds p, the remaining tokens are cut off from the probability space. \n",
        "\n",
        "In other words, instead of sampling only from the most likely K words, in Top-p sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability p. The probability mass is then redistributed among this set of words. This way, the size of the set of words (a.k.a the number of words in the set) can dynamically increase and decrease according to the next word's probability distribution.\n",
        "\n",
        "<img src=\"https://huggingface.co/blog/assets/02_how-to-generate/top_p_sampling.png\" alt=\"drawing\" width=\"500\"/>\n",
        "\n",
        "*In this way, we still avoid sampling egregiously wrong tokens, but preserve variety when the highest scoring tokens have low confidence.*\n",
        "\n",
        "**!!Important** : For our case, after extensive testing, we found out that `top_p` will help in increasing the length of the generated answer but as top p is quite high`(0.9)`, increasing `top_k` will quickly result in going off topic.\n",
        "\n",
        "So in short, To generate diverse answers from DialoGPT, turn on the sampling, provide no history, increase the\n",
        "temperature slightly and keep top_k in the range of 100 while maximizing top_p.\n",
        "\n",
        "For detailed explanation, please refer to [HuggingFace](https://huggingface.co/blog/how-to-generate)\n",
        "\n",
        "----------------------------------------------------------\n",
        "***-> do_sample***: In its most basic form, sampling means randomly picking the next word according to its conditional probability distribution. So, as the name suggests, when sampling is off model will produce the same result over and over again without sampling other tokens from the distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrDE6J1G1AUz"
      },
      "source": [
        "### Sample Outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gX8d-PH49gGJ",
        "outputId": "e3bf727d-dd3d-49bc-835a-98753231b02a"
      },
      "source": [
        "# without using paraphrasing on the input sentence\n",
        "prompts = ['Do you mind providing your email address?'] # The input prompt | Specify the input prompts as a list of strings\n",
        "\n",
        "# Note: By default the top_p value will be a range\n",
        "# If you want to use an absolute value for top_p\n",
        "# just replace the tuple with the absolute value and set\n",
        "# the 'top_p_step = None'\n",
        "\n",
        "# Note, when using range of top_p, the total number of paraphrases\n",
        "# generated will be:-\n",
        "'''total_responses = num_responses * (top_p[1]/top_p_step - (top_p[0]/top_p_step))'''\n",
        "\n",
        "# initialize the hyper-parameters\n",
        "hyperParameters = {'do_sample':True,\n",
        "                   'top_k':100,\n",
        "                   'top_p':(0.5,10),\n",
        "                   'top_p_step':0.5,\n",
        "                   'temperature':0.8}\n",
        "\n",
        "\n",
        "num_responses = 20\n",
        "out = get_answer(prompts, hyperParameters,num_responses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Do you mind providing your email address?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9PflDWKE5e5"
      },
      "source": [
        "output = createCSV(out,'/content','question_answer_pairs.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "3VcOvpCrnd5n",
        "outputId": "91c4f044-f7bc-4581-ed24-f938fc16a808"
      },
      "source": [
        "output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input_sentences</th>\n",
              "      <th>output_sentences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Do you mind providing your email address?</td>\n",
              "      <td>Sent you a PM.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Do you mind providing your email address?</td>\n",
              "      <td>I sent you a message, thanks!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Do you mind providing your email address?</td>\n",
              "      <td>Sure, I sent you a PM.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Do you mind providing your email address?</td>\n",
              "      <td>I do mind, and it's a gmail, if that helps.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Do you mind providing your email address?</td>\n",
              "      <td>yes, just sent you a PM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>375</th>\n",
              "      <td>Do you mind providing your email address?</td>\n",
              "      <td>Sure, just PM it to me.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>376</th>\n",
              "      <td>Do you mind providing your email address?</td>\n",
              "      <td>PMing you!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>377</th>\n",
              "      <td>Do you mind providing your email address?</td>\n",
              "      <td>yeah I sent you a PM.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>378</th>\n",
              "      <td>Do you mind providing your email address?</td>\n",
              "      <td>sent via PM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>379</th>\n",
              "      <td>Do you mind providing your email address?</td>\n",
              "      <td>Send an email to the email, and I will send th...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>308 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                               input_sentences                                   output_sentences\n",
              "0    Do you mind providing your email address?                                     Sent you a PM.\n",
              "1    Do you mind providing your email address?                      I sent you a message, thanks!\n",
              "2    Do you mind providing your email address?                             Sure, I sent you a PM.\n",
              "3    Do you mind providing your email address?        I do mind, and it's a gmail, if that helps.\n",
              "4    Do you mind providing your email address?                            yes, just sent you a PM\n",
              "..                                         ...                                                ...\n",
              "375  Do you mind providing your email address?                            Sure, just PM it to me.\n",
              "376  Do you mind providing your email address?                                         PMing you!\n",
              "377  Do you mind providing your email address?                              yeah I sent you a PM.\n",
              "378  Do you mind providing your email address?                                        sent via PM\n",
              "379  Do you mind providing your email address?  Send an email to the email, and I will send th...\n",
              "\n",
              "[308 rows x 2 columns]"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ohe1u_dEXLH"
      },
      "source": [
        "unique = []\n",
        "for (sentences,_) in out:\n",
        "  for sentence in sentences:\n",
        "    if sentence not in unique:\n",
        "      unique.append(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ld1pvoWxLyj8",
        "outputId": "5cc22417-156f-461a-9607-61e0ddaf15d8"
      },
      "source": [
        "# Output directly taken from pegasus (num_sequences = 10, beam = 20, chaining = False)\n",
        "# Right now, for testing, I have copied the output from other notebook to this notebook\n",
        "# I will merge both once everything is finalized\n",
        "\n",
        "prompts = ['Do you want to provide your email address?',\n",
        "   'Do you want to give your email address?',\n",
        "   'Are you willing to give your email address?',\n",
        "   'Are you willing to provide your email address?',\n",
        "   'Do you want to provide an email address?',\n",
        "   'Do you want to send an email?',\n",
        "   'Do you want your email address to be public?',\n",
        "   'Do you want to use your email address?',\n",
        "   'Do you have an email address?',\n",
        "   'Is it okay to give your email address?']\n",
        "# You can also specify multiple types of questions above\n",
        "\n",
        "# Note: By default the top_p value will be a range\n",
        "# If you want to use an absolute value for top_p\n",
        "# just replace the tuple with the absolute value and set\n",
        "# the 'top_p_step = None'\n",
        "\n",
        "# Note, when using range of top_p, the total number of paraphrased\n",
        "# generated will be:-\n",
        "'''total_responses = num_responses * (top_p[1]/top_p_step - (top_p[0]/top_p_step))'''\n",
        "\n",
        "# initialize the hyper-parameters\n",
        "hyperParameters = {'no_repeat_ngram_size':3,\n",
        "                   'do_sample':True,\n",
        "                   'top_k':100,\n",
        "                   'top_p':(0.5,10),\n",
        "                   'top_p_step':0.5,\n",
        "                   'temperature':0.8}\n",
        "\n",
        "\n",
        "num_responses = 10\n",
        "out = get_answer(questions_paraphrased, hyperParameters,num_responses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Do you want to provide your email address?\n",
            "Do you want to give your email address?\n",
            "Are you willing to give your email address?\n",
            "Are you willing to provide your email address?\n",
            "Do you want to provide an email address?\n",
            "Do you want to send an email?\n",
            "Do you want your email address to be public?\n",
            "Do you want to use your email address?\n",
            "Do you have an email address?\n",
            "Is it okay to give your email address?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "RhjqNISHvCwP",
        "outputId": "9149632c-5995-4f04-9d90-1998ab44152b"
      },
      "source": [
        "createCSV(out,'/content','question_answer_pairs_1.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input_sentences</th>\n",
              "      <th>output_sentences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Do you want to provide your email address?</td>\n",
              "      <td>I sent you a PM.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Do you want to provide your email address?</td>\n",
              "      <td>Yeah sure!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Do you want to provide your email address?</td>\n",
              "      <td>No, no, no. I just want to know what's going o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Do you want to provide your email address?</td>\n",
              "      <td>Thank you, I'll try that and see if the fix works</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Do you want to provide your email address?</td>\n",
              "      <td>Yes. Please. Let me know. I need to know becau...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1895</th>\n",
              "      <td>Is it okay to give your email address?</td>\n",
              "      <td>As long as it makes sense, yup.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1896</th>\n",
              "      <td>Is it okay to give your email address?</td>\n",
              "      <td>Sure, why not. Do you want anything in return?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1897</th>\n",
              "      <td>Is it okay to give your email address?</td>\n",
              "      <td>I just wanted to let you know, you're amazing ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1898</th>\n",
              "      <td>Is it okay to give your email address?</td>\n",
              "      <td>Feel free.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1899</th>\n",
              "      <td>Is it okay to give your email address?</td>\n",
              "      <td>I sent an email.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1688 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 input_sentences                                   output_sentences\n",
              "0     Do you want to provide your email address?                                   I sent you a PM.\n",
              "1     Do you want to provide your email address?                                         Yeah sure!\n",
              "2     Do you want to provide your email address?  No, no, no. I just want to know what's going o...\n",
              "3     Do you want to provide your email address?  Thank you, I'll try that and see if the fix works\n",
              "4     Do you want to provide your email address?  Yes. Please. Let me know. I need to know becau...\n",
              "...                                          ...                                                ...\n",
              "1895      Is it okay to give your email address?                    As long as it makes sense, yup.\n",
              "1896      Is it okay to give your email address?     Sure, why not. Do you want anything in return?\n",
              "1897      Is it okay to give your email address?  I just wanted to let you know, you're amazing ...\n",
              "1898      Is it okay to give your email address?                                         Feel free.\n",
              "1899      Is it okay to give your email address?                                   I sent an email.\n",
              "\n",
              "[1688 rows x 2 columns]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJZIHqBDWqbI"
      },
      "source": [
        "\n",
        "\n",
        "unique = []\n",
        "for (sentences,_) in out:\n",
        "  for sentence in sentences:\n",
        "    if sentence not in unique:\n",
        "      unique.append(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBAxRMPK3pcs"
      },
      "source": [
        "## GPT-J"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8STiUB3A3sKU",
        "outputId": "cd215c0d-06ed-4509-c749-1ca283188502"
      },
      "source": [
        "!pip install gptj"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gptj\n",
            "  Downloading gptj-2.2.5-py3-none-any.whl (14 kB)\n",
            "Collecting cryptography\n",
            "  Downloading cryptography-35.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 14.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gptj) (2.23.0)\n",
            "Collecting ProfanityDetector\n",
            "  Downloading ProfanityDetector-0.2-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->gptj) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->gptj) (2.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gptj) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gptj) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gptj) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gptj) (2021.5.30)\n",
            "Installing collected packages: ProfanityDetector, cryptography, gptj\n",
            "Successfully installed ProfanityDetector-0.2 cryptography-35.0.0 gptj-2.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unjWSPfe6CGb"
      },
      "source": [
        "from GPTJ.Basic_api import SimpleCompletion\n",
        "\n",
        "prompt = \"why do you want my email address?\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDAm03FS6Lgp"
      },
      "source": [
        "max_length = 100\n",
        "temperature = 0.09\n",
        "top_probability = 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGd1mE1w6S4Y"
      },
      "source": [
        "query = SimpleCompletion(prompt, length=max_length, t=temperature, top=top_probability)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7HepHZF6UQK",
        "outputId": "df6759de-3ec1-4ea4-928e-c0e2fb42e50e"
      },
      "source": [
        "query.simple_completion()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "I am a human being, not a spam bot.\n",
            "\n",
            "I am a human being, not a spam bot.\n",
            "\n",
            "I am a human being, not a spam bot.\n",
            "\n",
            "I am a human being, not a spam bot.\n",
            "\n",
            "I am a human being, not a spam bot.\n",
            "\n",
            "I am a human being, not a spam bot.\n",
            "\n",
            "I am a human being, not a spam bot.\n",
            "\n",
            "I am a human being, not\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\nI am a human being, not a spam bot.\\n\\nI am a human being, not a spam bot.\\n\\nI am a human being, not a spam bot.\\n\\nI am a human being, not a spam bot.\\n\\nI am a human being, not a spam bot.\\n\\nI am a human being, not a spam bot.\\n\\nI am a human being, not a spam bot.\\n\\nI am a human being, not'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsafgI_k6WjW"
      },
      "source": [
        "from GPTJ.gptj_api import Completion\n",
        "context = \"chatbot\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUkjeHHG6rVZ"
      },
      "source": [
        "examples = {\n",
        "    \"Do you mind providing your email address?\":\"Why do you want my email address?\",\n",
        "    \"Do you mind providing your email address?\": \"Do I have to?\",\n",
        "    \"Are you interested in a software engineer role?\": \"Are you interested in a software engineer role?\",\n",
        "    \"Are you interested in a software engineer role?\": \"Yes, I would love to work as a software engineer.\",\n",
        "    \"Would you mind sharing your CV\": \"Absolutely not. Here it is.\",\n",
        "    \"What is your name?\": \"My full name is M.Mehyar Ali\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "safX52l07tSW"
      },
      "source": [
        "context_setting = Completion(context, examples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p78nXQMl7vf3"
      },
      "source": [
        "prompt = \"Would you mind sharing your phone number?\"\n",
        "temperature = 0.09\n",
        "top_probability = 1.0\n",
        "response = context_setting.completion(prompt,\n",
        "              temperature=temperature,\n",
        "              top_p=top_probability)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgvVSIE18LB-",
        "outputId": "29cfe766-0032-4f50-ce99-1e06b5261dcb"
      },
      "source": [
        "print(response)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm not sure if this is the right place to ask this, but I'm not sure where else to ask.\n",
            "I'm a new user of Ubuntu and I'm having a problem with my phone. I have a Samsung Galaxy S3 and I'm using the latest version of Ubuntu. I'm not sure if this is the right place to ask this, but I'm not sure where else to ask.\n",
            "I'm having a problem with my phone. I have a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5i152ne8Ni0"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}